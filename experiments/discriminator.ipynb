{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"..\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discriminator\n",
    "\n",
    "I will first try the discriminator architecture on a generated, easy to classify data set. Two classes are generated, with 100-dimensional feature vectors (x). Feature vectors for the first class are randomnly generated using a uniform distribuition, while feature vectors for the second class are generated by adding $1.0$ to a randomly generated number from a uniform distribuition.\n",
    "\n",
    "I will first see the impact of using one-hot encodings as labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decide which device we want to run on\n",
    "\n",
    "ngpu = 1\n",
    "\n",
    "device = torch.device(\"cuda:0\" if (torch.cuda.is_available() and ngpu > 0) else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binary classes\n",
    "\n",
    "For the binary scenario, we will use the Binary Cross-Entropy loss function. Output is a single float. If closer to 0, the predicted class will be class 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>vector</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>[1.982117870852664, 1.5483193055178424, 1.8743...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>[1.418715409949313, 1.7365551148618708, 1.8584...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>[0.3888934328828856, 0.4883907022483611, 0.519...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>[0.5702234190321228, 0.411709502422555, 0.1705...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>[1.1901701568991827, 1.3838762314392503, 1.304...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   class                                             vector\n",
       "0      1  [1.982117870852664, 1.5483193055178424, 1.8743...\n",
       "1      1  [1.418715409949313, 1.7365551148618708, 1.8584...\n",
       "2      0  [0.3888934328828856, 0.4883907022483611, 0.519...\n",
       "3      0  [0.5702234190321228, 0.411709502422555, 0.1705...\n",
       "4      1  [1.1901701568991827, 1.3838762314392503, 1.304..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load sample vectors directly from pickle\n",
    "\n",
    "import os, pickle\n",
    "\n",
    "from settings import ROOT_DIR\n",
    "\n",
    "data = pickle.load(open(os.path.join(ROOT_DIR, 'static', 'pickles', 'discriminators', 'sample-vectors.pickle'), 'rb'))\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length: 80000 class 0: 50.286249999999995 % class 1: 49.713750000000005 %\n",
      "length: 20000 class 0: 50.78 % class 1: 49.220000000000006 %\n"
     ]
    }
   ],
   "source": [
    "# train/test split\n",
    "\n",
    "test_index = int(len(data) * 0.8)\n",
    "\n",
    "train = data[:test_index]\n",
    "test = data[test_index:]\n",
    "\n",
    "print('length: %s class 0: %s %% class 1: %s %%' % (len(train), sum(train['class']) / len(train) * 100, (len(train) - sum(train['class'])) / len(train) * 100))\n",
    "print('length: %s class 0: %s %% class 1: %s %%' % (len(test), sum(test['class']) / len(test) * 100, (len(test) - sum(test['class'])) / len(test) * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create our data tensors\n",
    "\n",
    "import torch\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "x_train = torch.tensor(np.vstack(train['vector']), dtype=torch.float, device=device)\n",
    "y_train = torch.tensor(np.vstack(train['class']), dtype=torch.float, device=device)\n",
    "x_test = torch.tensor(np.vstack(test['vector']), dtype=torch.float, device=device)\n",
    "y_test = torch.tensor(np.vstack(test['class']), dtype=torch.float, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model definition\n",
    "\n",
    "input_dimension = len(data['vector'][0])\n",
    "\n",
    "class Discriminator(torch.nn.Module):\n",
    "    def __init__(self, ngpu):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.ngpu = ngpu\n",
    "        self.main = torch.nn.Sequential(\n",
    "            # state size. input_dimension\n",
    "            torch.nn.Linear(input_dimension, 75),\n",
    "            # state size. 75\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(75, 25),\n",
    "            # state size. 25\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(25, 1),\n",
    "            torch.nn.Sigmoid() # 0 < output_value < 1\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.main(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discriminator(\n",
      "  (main): Sequential(\n",
      "    (0): Linear(in_features=100, out_features=75, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=75, out_features=25, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=25, out_features=1, bias=True)\n",
      "    (5): Sigmoid()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "loss_fn = torch.nn.BCELoss()\n",
    "\n",
    "learning_rate = 1e-1\n",
    "\n",
    "net = Discriminator(ngpu=ngpu)\n",
    "net.to(device)\n",
    "\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple accuracy metric: (TP + TN) / (TP + TN + FP + FN)\n",
    "\n",
    "def accuracy():\n",
    "    y_pred = net(x_test)\n",
    "    predicted_classes = y_pred > 0.5\n",
    "    return (predicted_classes.int() == y_test.int()).sum().float() / float(len(predicted_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 t: 0 loss: 0.6231658458709717 accuracy: tensor(0.5078, device='cuda:0')\n",
      "epoch: 1 t: 10 loss: 0.5571004748344421 accuracy: tensor(0.5078, device='cuda:0')\n",
      "epoch: 2 t: 20 loss: 0.49954453110694885 accuracy: tensor(0.5119, device='cuda:0')\n",
      "epoch: 3 t: 30 loss: 0.4389975070953369 accuracy: tensor(0.8087, device='cuda:0')\n",
      "epoch: 4 t: 40 loss: 0.3772467374801636 accuracy: tensor(0.9963, device='cuda:0')\n",
      "epoch: 5 t: 50 loss: 0.31594687700271606 accuracy: tensor(1., device='cuda:0')\n",
      "epoch: 6 t: 60 loss: 0.2574044466018677 accuracy: tensor(1., device='cuda:0')\n",
      "epoch: 7 t: 70 loss: 0.20526215434074402 accuracy: tensor(1., device='cuda:0')\n",
      "epoch: 8 t: 80 loss: 0.1615338921546936 accuracy: tensor(1., device='cuda:0')\n",
      "epoch: 9 t: 90 loss: 0.1266518235206604 accuracy: tensor(1., device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# training\n",
    "\n",
    "epoch_size = 10\n",
    "epochs = 10\n",
    "\n",
    "max_accuracy = 0\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for t in range(epoch_size):\n",
    "        y_pred = net(x_train)\n",
    "        loss = loss_fn(y_pred, y_train)\n",
    "        net.zero_grad()\n",
    "        loss.backward()\n",
    "        with torch.no_grad():\n",
    "            for param in net.parameters():\n",
    "                param.data -= learning_rate * param.grad\n",
    "    current_accuracy = accuracy()\n",
    "    if current_accuracy < max_accuracy:\n",
    "        break\n",
    "    else:\n",
    "        print('epoch: %s t: %s loss: %s accuracy: %s' % (epoch, epoch * epoch_size, loss.item(), current_accuracy))\n",
    "        max_accuracy = current_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-hot encoded\n",
    "\n",
    "For this scenario, we will use the Cross-Entropy loss function. Output is a float tuple with the probability for each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>vector</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[0, 1]</td>\n",
       "      <td>[1.982117870852664, 1.5483193055178424, 1.8743...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[0, 1]</td>\n",
       "      <td>[1.418715409949313, 1.7365551148618708, 1.8584...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[1, 0]</td>\n",
       "      <td>[0.3888934328828856, 0.4883907022483611, 0.519...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[1, 0]</td>\n",
       "      <td>[0.5702234190321228, 0.411709502422555, 0.1705...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[0, 1]</td>\n",
       "      <td>[1.1901701568991827, 1.3838762314392503, 1.304...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    class                                             vector\n",
       "0  [0, 1]  [1.982117870852664, 1.5483193055178424, 1.8743...\n",
       "1  [0, 1]  [1.418715409949313, 1.7365551148618708, 1.8584...\n",
       "2  [1, 0]  [0.3888934328828856, 0.4883907022483611, 0.519...\n",
       "3  [1, 0]  [0.5702234190321228, 0.411709502422555, 0.1705...\n",
       "4  [0, 1]  [1.1901701568991827, 1.3838762314392503, 1.304..."
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load sample vectors directly from pickle\n",
    "\n",
    "import os, pickle\n",
    "\n",
    "from settings import ROOT_DIR\n",
    "\n",
    "data_onehot = pickle.load(open(os.path.join(ROOT_DIR, 'static', 'pickles', 'discriminators', 'sample-vectors-onehot.pickle'), 'rb'))\n",
    "data_onehot.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length: 80000 class 0: 49.713750000000005 % class 1: 50.286249999999995 %\n",
      "length: 20000 class 0: 49.220000000000006 % class 1: 50.78 %\n"
     ]
    }
   ],
   "source": [
    "# train/test split\n",
    "\n",
    "test_index_onehot = int(len(data_onehot) * 0.8)\n",
    "\n",
    "train_onehot = data_onehot[:test_index_onehot]\n",
    "test_onehot = data_onehot[test_index_onehot:]\n",
    "\n",
    "print('length: %s class 0: %s %% class 1: %s %%' % (len(train_onehot), sum(train_onehot['class'].apply(lambda x: x[0])) / len(train_onehot) * 100, (len(train_onehot) - sum(train_onehot['class'].apply(lambda x: x[0]))) / len(train_onehot) * 100))\n",
    "print('length: %s class 0: %s %% class 1: %s %%' % (len(test_onehot), sum(test_onehot['class'].apply(lambda x: x[0])) / len(test_onehot) * 100, (len(test_onehot) - sum(test_onehot['class'].apply(lambda x: x[0]))) / len(test_onehot) * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create our data tensors\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "x_train_onehot = torch.tensor(np.vstack(train_onehot['vector']), dtype=torch.float, device=device)\n",
    "y_train_onehot = torch.tensor(np.vstack(train_onehot['class']), dtype=torch.float, device=device)\n",
    "x_test_onehot = torch.tensor(np.vstack(test_onehot['vector']), dtype=torch.float, device=device)\n",
    "y_test_onehot = torch.tensor(np.vstack(test_onehot['class']), dtype=torch.float, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiscriminatorOnehot(torch.nn.Module):\n",
    "    def __init__(self, ngpu):\n",
    "        super(DiscriminatorOnehot, self).__init__()\n",
    "        self.ngpu = ngpu\n",
    "        self.main = torch.nn.Sequential(\n",
    "            # state size. input_dimension\n",
    "            torch.nn.Linear(input_dimension, 75),\n",
    "            # state size. 75\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(75, 25),\n",
    "            # state size. 25\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(25, 2), # binary one-hot encoding vector\n",
    "            torch.nn.Softmax(dim=1) # values must sum 1 on dimension 1, that is for the two dimensional outputs for each example\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.main(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DiscriminatorOnehot(\n",
      "  (main): Sequential(\n",
      "    (0): Linear(in_features=100, out_features=75, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=75, out_features=25, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=25, out_features=2, bias=True)\n",
      "    (5): Softmax()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "learning_rate = 1e-1\n",
    "\n",
    "net = DiscriminatorOnehot(ngpu=0)\n",
    "net.to(device)\n",
    "\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.nn.CrossEntropyLoss takes the output vectors (a float for each class indicating the probability of belonging to that class - only one class per example) and a single target int \n",
    "# that represents the class label. Thus, we take the argmax of the one-hot encoded vectors to get a class id.\n",
    "\n",
    "def accuracy_onehot():\n",
    "    y_pred = net(x_test_onehot)\n",
    "    predicted_classes = y_pred.argmax(1)\n",
    "    return (predicted_classes == y_test_onehot.argmax(1)).sum().float() / float(len(predicted_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 t: 0 loss: 0.614982545375824 accuracy: tensor(0.5078, device='cuda:0')\n",
      "epoch: 1 t: 10 loss: 0.5875807404518127 accuracy: tensor(0.5078, device='cuda:0')\n",
      "epoch: 2 t: 20 loss: 0.5639144778251648 accuracy: tensor(0.5616, device='cuda:0')\n",
      "epoch: 3 t: 30 loss: 0.539263904094696 accuracy: tensor(0.9245, device='cuda:0')\n",
      "epoch: 4 t: 40 loss: 0.5145022869110107 accuracy: tensor(0.9982, device='cuda:0')\n",
      "epoch: 5 t: 50 loss: 0.4899929165840149 accuracy: tensor(1., device='cuda:0')\n",
      "epoch: 6 t: 60 loss: 0.4663316309452057 accuracy: tensor(1., device='cuda:0')\n",
      "epoch: 7 t: 70 loss: 0.4442014992237091 accuracy: tensor(1., device='cuda:0')\n",
      "epoch: 8 t: 80 loss: 0.4242170751094818 accuracy: tensor(1., device='cuda:0')\n",
      "epoch: 9 t: 90 loss: 0.4069361686706543 accuracy: tensor(1., device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "epoch_size = 10\n",
    "epochs = 10\n",
    "\n",
    "max_accuracy = 0\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for t in range(epoch_size):\n",
    "        y_pred = net(x_train_onehot)\n",
    "        loss = loss_fn(y_pred, y_train_onehot.argmax(1))\n",
    "        net.zero_grad()\n",
    "        loss.backward()\n",
    "        with torch.no_grad():\n",
    "            for param in net.parameters():\n",
    "                param.data -= learning_rate * param.grad\n",
    "    current_accuracy = accuracy_onehot()\n",
    "    if current_accuracy < max_accuracy:\n",
    "        break\n",
    "    else:\n",
    "        print('epoch: %s t: %s loss: %s accuracy: %s' % (epoch, epoch * epoch_size, loss.item(), current_accuracy))\n",
    "        max_accuracy = current_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The one-hot encoding approach seems working better for this test task, but we should check with the complete architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset class and package model definition\n",
    "\n",
    "Let's create a tidy workflow for this classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Dataset class\n",
    "\n",
    "import os\n",
    "\n",
    "from src.datasets.sample_vectors import SampleVectorDataset\n",
    "\n",
    "pickle_dir = os.path.join(ROOT_DIR, 'static', 'pickles', 'discriminators', 'sample-vectors.pickle')\n",
    "\n",
    "dataset = SampleVectorDataset(pickle_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.3888934328828856,\n",
       " 0.4883907022483611,\n",
       " 0.5196878749250333,\n",
       " 0.383532709936366,\n",
       " 0.19934792760457365,\n",
       " 0.4443464931636203,\n",
       " 0.56018541815734,\n",
       " 0.9246406702715995,\n",
       " 0.08085428787325377,\n",
       " 0.921788140686779]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[2][0][:10]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
