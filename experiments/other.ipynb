{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TripletNetwork(\n",
       "  (embedding_network): ConvolutionalNetwork(\n",
       "    (convolution_1): Conv2d(3, 6, kernel_size=(5, 5), stride=(1, 1))\n",
       "    (convolution_2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
       "    (convolution_3): Conv2d(16, 20, kernel_size=(4, 4), stride=(1, 1))\n",
       "    (fully_connected_1): Linear(in_features=16820, out_features=15000, bias=True)\n",
       "    (fully_connected_2): Linear(in_features=15000, out_features=1200, bias=True)\n",
       "    (fully_connected_3): Linear(in_features=1200, out_features=125, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os, pickle, torch\n",
    "\n",
    "from settings import ROOT_DIR\n",
    "\n",
    "resume = '19-05-08T20-18'\n",
    "start_epoch = 1\n",
    "\n",
    "checkpoint_directory = os.path.join(ROOT_DIR, 'static', 'checkpoints', 'triplet_cnn', resume)\n",
    "\n",
    "net = torch.load(os.path.join(checkpoint_directory, '_net_%s.pth' % start_epoch))\n",
    "net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data.dataloader import default_collate\n",
    "\n",
    "from src.datasets import get_dataset\n",
    "\n",
    "dataset_name = 'sketchy_test_photos_triplets'\n",
    "batch_size = 1\n",
    "workers = 4\n",
    "collate = default_collate\n",
    "\n",
    "dataset = get_dataset(dataset_name)\n",
    "\n",
    "data_loader = torch.utils.data.DataLoader(\n",
    "    dataset, batch_size=batch_size, shuffle=True,\n",
    "    num_workers=workers, collate_fn=collate\n",
    ")\n",
    "\n",
    "real_batch = next(iter(data_loader))\n",
    "\n",
    "query_image = real_batch[0][0]\n",
    "query_class = real_batch[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-4.8405, -4.8691, -4.7285, -4.9261, -4.8282, -4.8584, -4.9078, -4.8530,\n",
       "         -4.7228, -4.8582, -4.7934, -4.7548, -4.8168, -4.7971, -4.9568, -4.8788,\n",
       "         -4.8110, -4.9251, -4.8131, -4.9528, -4.8160, -4.7920, -4.8033, -4.9407,\n",
       "         -4.8070, -4.7939, -4.8498, -4.9279, -4.7366, -4.7871, -4.7383, -4.8220,\n",
       "         -4.7530, -4.7975, -4.8418, -4.7156, -4.8193, -4.7632, -4.8207, -4.7986,\n",
       "         -4.7419, -4.8491, -4.8751, -4.8214, -4.7689, -4.8845, -4.7574, -4.7301,\n",
       "         -4.8514, -4.8190, -4.7895, -4.9910, -4.7128, -4.8877, -4.8052, -4.8583,\n",
       "         -4.7615, -4.9760, -4.9106, -4.8773, -4.8694, -4.8280, -4.7208, -4.8753,\n",
       "         -4.8422, -4.8293, -4.8958, -4.7478, -4.8037, -4.7191, -4.8631, -4.8977,\n",
       "         -4.8559, -4.7383, -4.9056, -4.8241, -4.7967, -4.7940, -4.8386, -4.8291,\n",
       "         -4.8083, -4.7643, -4.7819, -4.8458, -4.7980, -4.7950, -4.9019, -4.8353,\n",
       "         -4.7793, -4.8258, -4.8760, -4.9048, -4.8730, -4.9390, -4.9217, -4.8800,\n",
       "         -4.9042, -4.7426, -4.9213, -4.7261, -4.7681, -4.8247, -4.8789, -4.7182,\n",
       "         -4.9592, -4.7241, -4.7816, -4.8586, -4.7851, -4.8501, -4.8796, -4.8677,\n",
       "         -4.8004, -4.9022, -4.7186, -4.8156, -4.8035, -4.7893, -4.8046, -4.8561,\n",
       "         -4.8037, -4.9321, -4.9267, -4.8492, -4.9214]],\n",
       "       grad_fn=<LogSoftmaxBackward>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_vector = net.embedding_network(query_image)\n",
    "query_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ConvolutionalNetwork(\n",
      "  (convolution_1): Conv2d(3, 6, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (convolution_2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (convolution_3): Conv2d(16, 20, kernel_size=(4, 4), stride=(1, 1))\n",
      "  (fully_connected_1): Linear(in_features=16820, out_features=15000, bias=True)\n",
      "  (fully_connected_2): Linear(in_features=15000, out_features=1200, bias=True)\n",
      "  (fully_connected_3): Linear(in_features=1200, out_features=125, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from src.models.convolutional_network import ConvolutionalNetwork\n",
    "\n",
    "convnet = ConvolutionalNetwork()\n",
    "print(convnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-4.8118, -4.8128, -4.8104, -4.8181, -4.7960, -4.8277, -4.8574, -4.8383,\n",
       "         -4.8347, -4.8058, -4.8239, -4.8313, -4.8592, -4.8107, -4.8338, -4.8500,\n",
       "         -4.8273, -4.8491, -4.8414, -4.8232, -4.8450, -4.8199, -4.8098, -4.8470,\n",
       "         -4.8533, -4.8339, -4.8438, -4.8270, -4.8265, -4.8418, -4.8271, -4.8430,\n",
       "         -4.8264, -4.7970, -4.8502, -4.8211, -4.8189, -4.8087, -4.8228, -4.8006,\n",
       "         -4.8204, -4.8408, -4.8583, -4.8308, -4.8350, -4.8416, -4.8437, -4.8100,\n",
       "         -4.8268, -4.8064, -4.8228, -4.8432, -4.8121, -4.8323, -4.8584, -4.8114,\n",
       "         -4.8311, -4.8014, -4.7982, -4.8286, -4.8425, -4.7974, -4.8456, -4.8547,\n",
       "         -4.7996, -4.8247, -4.8004, -4.8343, -4.8485, -4.8541, -4.8573, -4.8163,\n",
       "         -4.8478, -4.8182, -4.8096, -4.8444, -4.8586, -4.8369, -4.8224, -4.8139,\n",
       "         -4.8097, -4.8360, -4.8020, -4.8419, -4.8052, -4.8041, -4.8172, -4.8002,\n",
       "         -4.8381, -4.8490, -4.8276, -4.8191, -4.8034, -4.8024, -4.8487, -4.8125,\n",
       "         -4.8034, -4.8192, -4.8409, -4.8214, -4.8506, -4.8495, -4.8238, -4.8203,\n",
       "         -4.8465, -4.8593, -4.8199, -4.8124, -4.8600, -4.8488, -4.8395, -4.8144,\n",
       "         -4.8531, -4.8158, -4.8362, -4.8075, -4.8642, -4.8222, -4.8272, -4.8190,\n",
       "         -4.8580, -4.8299, -4.8423, -4.8203, -4.8092]],\n",
       "       grad_fn=<LogSoftmaxBackward>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector = convnet(query_image)\n",
    "vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "-"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
